{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks Tutorial(GNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pytorch Geometric Framework\n",
    "- Understanding Message Passing Scheme in Pytorch Geometric.\n",
    "- Efficient graph data representations and paralleling minibatching graphs.\n",
    "- Showcase the implementation of **Graph Convolution Networks** (Kipf & Welling, [SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS](https://arxiv.org/abs/1609.02907), ICLR 2017), and you should implement **GraphSAGE** (Hamilton et al, [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216), NIPS 2017) in the lab based on message passing scheme.\n",
    "\n",
    "#### 2. Vertex Classification\n",
    "- Showcase a model developed based on our GCN implementation to do vertex classification on Cora dataset. \n",
    "- Develop a model with **your own** GraphSAGE (with mean/sum/max aggregation) implementation on the same dataset to get insights of difference.\n",
    "\n",
    "#### 3. Graph Classification\n",
    "- Implement **GINConv** (Xu et al, [HOW POWERFUL ARE GRAPH NEURAL NETWORKS?](https://arxiv.org/abs/1810.00826), ICLR 2019) on graph classification benchmark dataset (IMDB) and compare different aggregation functions (SUM/MEAN/MAX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up working environment\n",
    "\n",
    "For this tutorial you will need to train a large network, therefore we recommend you work with Google Colaboratory, which provides free GPU time. You will need a Google account to do so. Please log in to your account and go to the following page: https://colab.research.google.com. Then upload this notebook.For GPU support, go to \"Edit\" -> \"Notebook Settings\", and select \"Hardware accelerator\" as \"GPU\".You will need to install pytorch by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install torch-scatter torch-sparse torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Geometric Framework\n",
    "\n",
    "#### Generic Message Passing Scheme\n",
    "Generalizing the convolution operator to irregular domains is typically expressed as a *neighborhood aggregation* or *message passing* scheme.\n",
    "With $\\mathbf{x}^{(k-1)}_i \\in \\mathbb{R}^F$ denoting node features of node $i$ in layer $(k-1)$ and $\\mathbf{e}_{i,j} \\in \\mathbb{R}^D$ denoting (optional) edge features from node $i$ to node $j$, message passing graph neural networks can be described as\n",
    "\n",
    "$$\n",
    "  \\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{i,j}\\right) \\right)\n",
    "$$\n",
    "\n",
    "where $\\square$ denotes a differentiable, permutation invariant function, *e.g.*, sum, mean or max, and $\\gamma$ and $\\phi$ denote differentiable functions such as MLPs (Multi Layer Perceptrons).\n",
    "\n",
    "#### Graph data representations in PyG\n",
    "Given a *sparse* **Graph** $\\mathcal{G}=(\\mathbf{X}, (\\mathbf{I}, \\mathbf{E}))$ with **node features** $\\mathbf{X} \\in \\mathbb{R}^{|V| \\times F}$, **edge indices $\\mathbf{I} \\in \\{1, \\cdots, N\\}^{2 \\times |\\mathcal{E}|}$**, (optional) **edge features** $\\mathbf{E} \\in \\mathbb{R}^{|\\mathcal{E} \\times D|}$, it is described by an instance of class `torch_geometric.data.Data`, which holds the corresponding attributes.\n",
    "\n",
    "We show a simple example of an unweighted and directed graph with four nodes and three edges.\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"./figures/graph_data.png\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 1], edge_index=[2, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[2, 1, 3],\n",
    "                           [0, 0, 2]], dtype=torch.long)\n",
    "x = torch.tensor([[1], [1], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batching Graphs\n",
    "Neural networks are usually trained in a batch-wise fashion. Minibatch graphs can be efficiently dealt with to achieve parallelization over a mini-batch from creating sparse block diagnoal adjacency matrices and concatenating features and target matrices in the node dimension.\n",
    "\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"./figures/mini_batch_graph.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Message Passing Scheme in PyG\n",
    "\n",
    "PyTorch Geometric provides the `torch_geometric.nn.MessagePassing` base class, which helps in creating such kinds of message passing graph neural networks by automatically taking care of message propagation. The implementation is decoupled into **UPDATE**, **AGGREGATION**, **MESSAGE** functions as:\n",
    "$$\n",
    "    \\mathbf{x}_i^{(k)} = \\mathrm{UPDATE} \\left( \\mathbf{x}_i, , \\mathrm{AGGR}_{j \\in \\mathcal{N}(i)} \\, \\mathrm{MESSAGE}^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{i,j}\\right) \\right)    \n",
    "$$\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"./figures/message_passing.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the GCN layer (lecture)\n",
    "\n",
    "The graph convolutional operator introduced by Kipf & Welling (ICLR 2017) is defined as\n",
    "$$\n",
    "        \\mathbf{X}^{k} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X}^{k-1} \\mathbf{\\Theta},\n",
    "$$\n",
    "where $\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}$ denotes the adjacency matrix with inserted self-loops and\n",
    "$\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}$ its diagonal degree matrix. It is equivalent as:\n",
    "$$\n",
    "\\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{deg(j)}} \\cdot \\left( \\mathbf{x}_j^{(k-1)}\\mathbf{\\Theta} \\right),\n",
    "$$\n",
    "\n",
    "where neighboring node features are first transformed by a weight matrix $\\mathbf{\\Theta}$, normalized by their degree, and finally summed up.\n",
    "This formula can be divided into the following steps:\n",
    "\n",
    "1. Add self-loops to the adjacency matrix.\n",
    "2. Linearly transform node feature matrix.\n",
    "3. Normalize node features.\n",
    "4. Sum up neighboring node features.\n",
    "5. Return new node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "        \n",
    "def add_self_loops(edge_index, num_nodes=None):\n",
    "    loop_index = torch.arange(0, num_nodes, dtype=torch.long,\n",
    "                              device=edge_index.device)\n",
    "    loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
    "\n",
    "    edge_index = torch.cat([edge_index, loop_index], dim=1)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def degree(index, num_nodes=None, dtype=None):\n",
    "    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)\n",
    "    return out.scatter_add_(0, index, out.new_ones((index.size(0))))\n",
    "        \n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        zeros(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        \n",
    "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages.\n",
    "\n",
    "        return self.propagate(edge_index, x=x)\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        # Step 3: Normalize node features.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################              \n",
    "        \n",
    "\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing GraphSAGE (lab)\n",
    "\n",
    "The algorithm of GraphSAGE (*Inductive Representation Learning on Large Graphs (NIPS 2017)*) embedding generation is described as:\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"./figures/graphsage.png\"></p>\n",
    "\n",
    "You are required to implement this algortihm with **MEAN/SUM/MAX** AGGREGATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "import math\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    \"\"\"Initializes a tensor with uniform distribution.\"\"\"\n",
    "    bound = 1.0 / math.sqrt(size)\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "class SAGEConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    GraphSAGE layer implementation.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        aggr (str, optional): Aggregation method ('add', 'mean', 'max'). Default is 'mean'.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, aggr='mean'):\n",
    "        super(SAGEConv, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Weight parameter for feature transformation\n",
    "        self.weight = Parameter(torch.Tensor(2 * in_channels, out_channels))\n",
    "\n",
    "        # Reset parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Resets parameters using uniform initialization.\"\"\"\n",
    "        uniform(self.weight.size(0), self.weight)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GraphSAGE layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Node feature matrix of shape [num_nodes, in_channels].\n",
    "            edge_index (torch.Tensor): Edge indices in COO format with shape [2, num_edges].\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Updated node features of shape [num_nodes, out_channels].\n",
    "        \"\"\"\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        \"\"\"\n",
    "        Constructs messages from neighboring nodes.\n",
    "\n",
    "        Args:\n",
    "            x_j (torch.Tensor): Neighbor features of shape [num_edges, in_channels].\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Message tensor of shape [num_edges, in_channels].\n",
    "        \"\"\"\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        \"\"\"\n",
    "        Updates node features after message aggregation.\n",
    "\n",
    "        Args:\n",
    "            aggr_out (torch.Tensor): Aggregated messages of shape [num_nodes, in_channels].\n",
    "            x (torch.Tensor): Original node features of shape [num_nodes, in_channels].\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Updated node features of shape [num_nodes, out_channels].\n",
    "        \"\"\"\n",
    "        # Concatenate original node features with aggregated features\n",
    "        aggr_out = torch.cat([x, aggr_out], dim=-1)\n",
    "\n",
    "        # Apply linear transformation\n",
    "        aggr_out = torch.matmul(aggr_out, self.weight)\n",
    "\n",
    "        # Normalize features\n",
    "        aggr_out = F.normalize(aggr_out, p=2, dim=-1)\n",
    "\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "path = osp.join(os.getcwd(), 'data', 'Cora')\n",
    "dataset = Planetoid(path, 'Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora()\n",
      "Number of graphs: 1\n",
      "Number of classes: 7\n",
      "Number of node features: 1433\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Define the dataset path dynamically\n",
    "def get_dataset_path(dataset_name='Cora', base_dir='data'):\n",
    "    \"\"\"Returns the dataset storage path.\"\"\"\n",
    "    return os.path.join(os.getcwd(), base_dir, dataset_name)\n",
    "\n",
    "# Load the dataset\n",
    "def load_dataset(dataset_name='Cora'):\n",
    "    \"\"\"\n",
    "    Loads the specified Planetoid dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load (e.g., 'Cora', 'Citeseer').\n",
    "        \n",
    "    Returns:\n",
    "        torch_geometric.data.Dataset: The loaded dataset.\n",
    "    \"\"\"\n",
    "    path = get_dataset_path(dataset_name)\n",
    "    return Planetoid(path, dataset_name)\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = load_dataset('Cora')\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Number of node features: {dataset.num_node_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7261, Test Accuracy: 0.785 ± 0.011, Duration: 5.545\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "runs = 10\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 0.0005\n",
    "early_stopping = 10\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Define the layers and initialization of the GCN model.\n",
    "        \n",
    "        Args:\n",
    "            dataset (torch_geometric.data.Dataset): The input graph dataset.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Define the GCN layers\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden)\n",
    "        self.conv2 = GCNConv(hidden, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset parameters of the layers.\"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            data (torch_geometric.data.Data): The input graph data containing node features and edge indices.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The log-softmax probabilities for each node.\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Apply the first GCN layer followed by ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        x = F.dropout(x, p=dropout, training=self.training)\n",
    "        \n",
    "        # Apply the second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Return log-softmax values for classification\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Running the model with hyperparameters and dataset\n",
    "run(dataset, Net(dataset).to(device), runs, epochs, lr, weight_decay, early_stopping)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
